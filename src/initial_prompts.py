"""
Generation 0 prompt creation for genetic algorithm.

This module creates the initial population of prompts that will evolve:
1. Generate N prompts using random LLM selection for diversity
2. Evaluate fitness with existing Phase 2 pipeline
3. Store in Couchbase prompts collection
4. Foundation for genetic algorithm evolution

All Generation 0 prompts have:
- generation=0
- type="initial" (NOT "immigrant")
- parents=None
- All tags: source="initial", parent_tag_guid=None

Used by: Evolution orchestrator, testing scripts
Creates: Initial population ready for GA evolution
Critical for: Establishing baseline fitness, providing genetic diversity

Related files:
- src/models.py: Prompt and PromptTag structures
- src/llm_clients.py: LLM API wrappers and generate_with_random_model()
- src/fitness_evaluator.py: Fitness evaluation pipeline
- src/couchbase_client.py: Database operations
"""

import uuid
import json
import re
import time
import random
from typing import List, Dict, Optional
import numpy as np

from src.models import Prompt, PromptTag
from src.ga_operators import JSONParseError
from src.llm_clients import generate_with_random_model
from src.fitness_evaluator import evaluate_prompt_fitness


def generate_initial_prompt(
    era: str,
    paragraph_id: str,
    paragraph_text: str,
    temperature: float = 1.0
) -> Prompt:
    """
    Generate a single Generation 0 prompt.

    Uses generate_with_random_model() to ensure model diversity across
    the population. Each prompt gets 5 tags (role, compression_target,
    fidelity, constraints, output) generated by an LLM.

    Process:
    1. Build generation prompt WITHOUT text context (Trust the LLM principle)
    2. Call random LLM with temperature=1.0 for creative diversity
    3. Parse JSON response (handles markdown blocks)
    4. Create Prompt with generation=0, type="initial"
    5. All tags get NEW guids, parent_tag_guid=None, source="initial"

    Args:
        era: Era identifier (e.g., "test-1", "mixed-1")
        paragraph_id: Source paragraph ID (stored as metadata, NOT used in generation)
        paragraph_text: NOT USED in generation (only stored for evaluation later)

    Returns:
        Unevaluated Prompt object (fitness=None until evaluation)

    Error Handling:
        FAIL LOUD - If JSON parsing fails or required tags are missing/empty,
        raises JSONParseError or ValueError. No fallback defaults.
    """
    # CRITICAL - Per "Trust the LLM Principle" in CLAUDE.md, this prompt intentionally
    # provides NO context about text type/domain. We want domain-agnostic compression
    # strategies, not specialized approaches (medical, legal, etc.) that fail across domains.
    # Providing sample text causes LLMs to create domain-specific prompts; we prevent that.
    generation_prompt = """Generate a semantic compression prompt with 5 sections.

GOAL: Achieve maximum compression while preserving core semantic content
OPTIMIZATION TARGET: Minimize token count (not word count)
USE CASE: Output will be used in retrieval systems, not human reading

Create these 5 sections:
1. ROLE: Establish expertise and task
2. COMPRESSION_TARGET: Specify compression strategy (NOT hard limits)
3. FIDELITY: What must be preserved (concepts, entities, relationships)
4. CONSTRAINTS: What to avoid (explanations, meta-commentary, filler)
5. OUTPUT: Format and style requirements

IMPORTANT CONSTRAINTS:
- DO NOT add hard word limits (e.g., "output exactly 45 words")
- DO NOT add hard sentence limits (e.g., "use maximum 3 sentences")
- DO NOT add hard token limits (e.g., "compress to 100 tokens")
- Focus on semantic compression STRATEGIES, not numeric targets

Respond with ONLY a JSON object:
{
  "role": "...",
  "compression_target": "...",
  "fidelity": "...",
  "constraints": "...",
  "output": "..."
}"""

    # Generate with random model (use configured temperature for diversity)
    response, model_used = generate_with_random_model(
        generation_prompt,
        temperature=temperature
    )

    # Parse JSON (handle markdown blocks from Claude)
    json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
    if json_match:
        json_str = json_match.group(1)
    else:
        # Try to find JSON object in response
        json_match = re.search(r'\{.*?\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
        else:
            json_str = response.strip()

    try:
        tags_dict = json.loads(json_str)
    except json.JSONDecodeError as e:
        # FAIL LOUD - do NOT use fallback defaults
        raise JSONParseError(
            f"Failed to parse initial prompt tags from {model_used}. "
            f"Raw response: {response[:300]}... "
            f"Extracted JSON: {json_str[:200]}... "
            f"JSON Error: {e}"
        ) from e

    # Validate all required keys present and non-empty
    required_keys = ["role", "compression_target", "fidelity", "constraints", "output"]
    for key in required_keys:
        if key not in tags_dict:
            raise ValueError(
                f"LLM returned missing '{key}' tag. "
                f"Model: {model_used}, Response: {response[:200]}..."
            )
        if not tags_dict[key].strip():
            raise ValueError(
                f"LLM returned empty '{key}' tag. "
                f"Model: {model_used}, Response: {response[:200]}..."
            )

    # Create Prompt object with Generation 0 structure
    return Prompt(
        prompt_id=str(uuid.uuid4()),
        generation=0,
        era=era,
        type="initial",  # NOT "immigrant"
        parents=None,    # No parents for Gen 0
        model_used=model_used,
        source_paragraph_id=paragraph_id,

        # 5 tags with NEW guids, no parents, source="initial", origin="initial"
        role=PromptTag(
            guid=str(uuid.uuid4()),
            text=tags_dict["role"],  # No fallback - already validated
            parent_tag_guid=None,
            source="initial",
            origin="initial"
        ),
        compression_target=PromptTag(
            guid=str(uuid.uuid4()),
            text=tags_dict["compression_target"],  # No fallback - already validated
            parent_tag_guid=None,
            source="initial",
            origin="initial"
        ),
        fidelity=PromptTag(
            guid=str(uuid.uuid4()),
            text=tags_dict["fidelity"],  # No fallback - already validated
            parent_tag_guid=None,
            source="initial",
            origin="initial"
        ),
        constraints=PromptTag(
            guid=str(uuid.uuid4()),
            text=tags_dict["constraints"],  # No fallback - already validated
            parent_tag_guid=None,
            source="initial",
            origin="initial"
        ),
        output=PromptTag(
            guid=str(uuid.uuid4()),
            text=tags_dict["output"],  # No fallback - already validated
            parent_tag_guid=None,
            source="initial",
            origin="initial"
        ),

        # Fitness fields - None until evaluation
        fitness=None,
        original_text=None,
        compressed_text=None,
        original_words=None,
        compressed_words=None,
        compression_ratio=None,
        quality_scores=None,
        quality_score_avg=None,
        survival_factor=None
    )


def generate_initial_population(
    era: str,
    population_size: int,
    couchbase_client
) -> List[Prompt]:
    """
    DEPRECATED: This function is no longer used.

    The paragraphs collection is deprecated. Current architecture uses the
    unstructured collection directly via corpus_sampler.py.

    Use create_generation_zero() instead, which calls select_evaluation_corpus()
    to get vetted chunks from the unstructured collection.

    Raises:
        NotImplementedError: Always raised to indicate deprecated function
    """
    raise NotImplementedError(
        "generate_initial_population() is deprecated. "
        "The 'paragraphs' collection is no longer used. "
        "Use create_generation_zero() instead, which queries the 'unstructured' collection "
        "via select_evaluation_corpus() in corpus_sampler.py."
    )


def store_prompt_with_fitness(
    prompt: Prompt,
    couchbase_client,
    compression_model: str,
    judge_models: Optional[List[str]] = None,
    paragraph_text: Optional[str] = None,
    use_token_metric: bool = False
) -> bool:
    """
    Evaluate and store a single prompt.

    Orchestrates the complete evaluation pipeline:
    1. Fetch paragraph text from Couchbase OR use provided paragraph_text
    2. Call evaluate_prompt_fitness() from Phase 2
    3. Update prompt object with fitness results
    4. Save to prompts collection

    Args:
        prompt: Unevaluated Prompt object
        couchbase_client: Connected CouchbaseClient instance
        compression_model: Model for compression ("openai"|"claude"|"gemini")
        judge_models: List of judge models (default: all 3)
        paragraph_text: Optional paragraph text (if not provided, fetches from DB)
        use_token_metric: If True, use token-based compression ratio for fitness

    Returns:
        True if successful, False if error occurred

    Error Handling:
        Catches all exceptions, prints error, returns False.
        Allows population creation to continue even if individual
        prompts fail. Caller tracks success rate.
    """
    if judge_models is None:
        judge_models = ["openai", "claude", "gemini"]

    try:
        # Get paragraph text (must be provided - unstructured collection is source)
        if paragraph_text is None:
            raise ValueError(
                f"paragraph_text must be provided for prompt {prompt.prompt_id}. "
                f"The paragraphs collection is deprecated. "
                f"Use select_evaluation_corpus() to get text from unstructured collection."
            )

        # Evaluate fitness using Phase 2 pipeline
        results = evaluate_prompt_fitness(
            prompt_object=prompt,
            paragraph_text=paragraph_text,
            compression_model=compression_model,
            judge_models=judge_models,
            use_token_metric=use_token_metric
        )

        # Update prompt with evaluation results
        prompt.original_text = results["original_text"]
        prompt.compressed_text = results["compressed_text"]
        prompt.original_words = results["original_words"]
        prompt.compressed_words = results["compressed_words"]
        prompt.compression_ratio = results["compression_ratio"]
        prompt.original_tokens = results["original_tokens"]
        prompt.compressed_tokens = results["compressed_tokens"]
        prompt.token_compression_ratio = results["token_compression_ratio"]
        prompt.quality_scores = results["quality_scores"]
        prompt.quality_score_avg = results["quality_score_avg"]
        prompt.survival_factor = results["survival_factor"]
        prompt.fitness = results["fitness"]

        # Save to generations collection with era-gen-id format
        doc_id = f"{prompt.era}-gen-{prompt.generation}-{prompt.prompt_id}"
        couchbase_client.save_document("generations", doc_id, prompt.to_dict())

        return True

    except Exception as e:
        print(f"Error storing {prompt.prompt_id}: {e}")
        return False


def create_generation_zero(
    era: str,
    population_size: int,
    compression_model: str,
    couchbase_client,
    use_token_metric: bool = False,
    prompt_temperature: float = 1.0,
    single_tag: bool = False
) -> Dict:
    """
    Orchestrate complete Generation 0 creation.

    This is the main entry point for creating an initial population.
    Handles the complete workflow from generation through evaluation
    to storage, with progress reporting and statistics.

    Process:
    1. Generate population (unevaluated prompts)
    2. Evaluate and store each (with progress updates)
    3. Calculate population statistics
    4. Return summary for logging/analysis

    Args:
        era: Era identifier (e.g., "test-1", "mixed-1")
        population_size: Number of prompts to create
        compression_model: Model for compression execution
        couchbase_client: Connected CouchbaseClient instance
        use_token_metric: If True, use token-based compression ratio for fitness

    Returns:
        Statistics dictionary:
        {
            "era": str,
            "generation": 0,
            "population_size": int,
            "success_count": int,
            "mean_fitness": float,
            "std_fitness": float,
            "median_fitness": float,
            "min_fitness": float,
            "max_fitness": float,
            "elapsed_seconds": float,
            "prompts_per_minute": float,
            "fitness_metric": str,  # "tokens" or "words"
            "compression_model": str
        }

    Performance:
        ~10 seconds per prompt (2s generation + 2s compression + 6s judging)
        Expected time:
        - 5 prompts: ~1 minute
        - 20 prompts: ~3-4 minutes
        - 100 prompts: ~15-17 minutes
    """
    fitness_metric = "tokens" if use_token_metric else "words"

    print(f"\n{'='*60}")
    print(f"CREATING GENERATION 0")
    print(f"{'='*60}")
    print(f"Era: {era}")
    print(f"Population: {population_size}")
    print(f"Compression Model: {compression_model}")
    print(f"Fitness Metric: {fitness_metric}")
    print(f"{'='*60}\n")

    start_time = time.time()

    # Step 1: Select evaluation corpus (vetted pool of chunks from unstructured)
    print(f"\n=== Selecting Evaluation Corpus ===")

    from src.corpus_sampler import select_evaluation_corpus
    import random

    evaluation_corpus = select_evaluation_corpus(
        couchbase_client=couchbase_client,
        corpus_size=20,
        min_words=550,
        max_words=650
    )
    corpus_ids = [p["chunk_id"] for p in evaluation_corpus]
    print(f"✓ Selected {len(evaluation_corpus)} vetted chunks for evaluation pool\n")

    # Step 2: Generate prompts (unevaluated)
    print(f"\n=== Generating {population_size} Initial Prompts ===")
    print(f"Era: {era}")
    print(f"Each prompt uses random chunk from evaluation corpus\n")

    prompts = []
    for i in range(population_size):
        # Randomly select chunk from vetted corpus for this prompt
        para = random.choice(evaluation_corpus)

        prompt = generate_initial_prompt(
            era=era,
            paragraph_id=para["chunk_id"],
            paragraph_text=para["text"],
            temperature=prompt_temperature
        )
        prompts.append(prompt)

        # Progress updates
        if (i + 1) % 10 == 0 or (i + 1) == population_size:
            print(f"  Generated {i + 1}/{population_size}...")

    print(f"✓ Generation complete\n")

    # Step 3: Evaluate and store each prompt
    print(f"=== Evaluating Fitness ===")
    print(f"Expected time: ~{population_size * 10 / 60:.1f} minutes")
    print(f"(~10 sec per prompt: 2s generation + 8s evaluation)")
    print(f"Each prompt randomly selects from vetted pool of {len(evaluation_corpus)} paragraphs\n")

    success_count = 0
    fitness_values = []

    for i, prompt in enumerate(prompts):
        print(f"[{i+1}/{population_size}] Evaluating {prompt.prompt_id[:8]}...")

        # Randomly select paragraph from vetted corpus
        para = random.choice(evaluation_corpus)

        if store_prompt_with_fitness(prompt, couchbase_client, compression_model, paragraph_text=para["text"], use_token_metric=use_token_metric):
            success_count += 1
            if prompt.fitness is not None:
                fitness_values.append(prompt.fitness)

        # Progress updates every 10 prompts
        if (i + 1) % 10 == 0:
            elapsed = time.time() - start_time
            remaining = (elapsed / (i + 1)) * (len(prompts) - (i + 1))
            print(f"\nProgress: {i+1}/{population_size}")
            print(f"  Elapsed: {elapsed/60:.1f} min")
            print(f"  Remaining: ~{remaining/60:.1f} min\n")

    elapsed_seconds = time.time() - start_time

    # Step 4: Calculate statistics
    if fitness_values:
        stats = {
            "era": era,
            "generation": 0,
            "population_size": population_size,
            "success_count": success_count,
            "mean_fitness": float(np.mean(fitness_values)),
            "std_fitness": float(np.std(fitness_values)),
            "median_fitness": float(np.median(fitness_values)),
            "min_fitness": float(np.min(fitness_values)),
            "max_fitness": float(np.max(fitness_values)),
            "elapsed_seconds": elapsed_seconds,
            "prompts_per_minute": (success_count / elapsed_seconds) * 60,
            "evaluation_corpus_ids": corpus_ids,
            # Operator counts (not applicable to Gen 0, but required for consistent schema)
            "elite_count": 0,
            "crossover_count": 0,
            "mutation_count": 0,
            "immigrant_count": 0,
            "evaluated_count": success_count,
            # Fitness evaluation metadata (NEW)
            "fitness_metric": fitness_metric,
            "compression_model": compression_model,
            "prompt_temperature": prompt_temperature,
            "single_tag": single_tag
        }
    else:
        stats = {
            "era": era,
            "generation": 0,
            "population_size": population_size,
            "success_count": 0,
            "mean_fitness": 0.0,
            "std_fitness": 0.0,
            "median_fitness": 0.0,
            "min_fitness": 0.0,
            "max_fitness": 0.0,
            "elapsed_seconds": elapsed_seconds,
            "prompts_per_minute": 0.0,
            "evaluation_corpus_ids": corpus_ids,
            # Operator counts (not applicable to Gen 0, but required for consistent schema)
            "elite_count": 0,
            "crossover_count": 0,
            "mutation_count": 0,
            "immigrant_count": 0,
            "evaluated_count": 0,
            # Fitness evaluation metadata (NEW)
            "fitness_metric": fitness_metric,
            "compression_model": compression_model,
            "prompt_temperature": prompt_temperature,
            "single_tag": single_tag
        }

    # Step 4: Store statistics to database
    from src.evolution import store_generation_stats

    print(f"\n[5/5] Storing statistics to database...")
    try:
        store_generation_stats(
            era=era,
            generation=0,
            stats=stats,
            couchbase_client=couchbase_client
        )
        print(f"✓ Statistics saved to generation_stats collection")
    except Exception as e:
        print(f"❌ CRITICAL ERROR storing generation_stats: {e}")
        raise  # Fail loud - don't continue with broken statistics

    # Step 5: Print summary
    print(f"\n{'='*60}")
    print(f"GENERATION 0 COMPLETE")
    print(f"{'='*60}")
    print(f"Success: {success_count}/{population_size}")
    print(f"Mean Fitness: {stats['mean_fitness']:.2f}")
    print(f"Std Dev: {stats['std_fitness']:.2f}")
    print(f"Range: [{stats['min_fitness']:.2f}, {stats['max_fitness']:.2f}]")
    print(f"Time: {elapsed_seconds/60:.1f} minutes")
    print(f"Rate: {stats['prompts_per_minute']:.1f} prompts/min")
    print(f"{'='*60}\n")

    return stats
